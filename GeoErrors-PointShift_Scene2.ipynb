{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeometricErrors - Point Shifting \n",
    "\n",
    "For KDD 2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Environment\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiang-ws1/.virtualenvs/geo_errors/lib/python3.7/site-packages/geopandas/_compat.py:88: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.8.1-CAPI-1.13.3). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Environment Configuration \"\"\"\n",
    "\n",
    "# Source of input tensors. \n",
    "data_path = '/data/GeometricErrors/aaai_data/scene02'\n",
    "\n",
    "# Source of original rasters, shapefiles.\n",
    "source_path ='/data/GeometricErrors/aaai_data/scene02'\n",
    "\n",
    "# Label of dataset used (Original = 1, New = 2)\n",
    "dataset_label = 2\n",
    "\n",
    "# Path to save EM Test as folder\n",
    "out_root_dir = '/data/GeometricErrors/tests'\n",
    "\n",
    "# Enables training with GPU\n",
    "use_gpu = True\n",
    "gpus = ['/gpu:0', '/gpu:1', '/gpu:2', '/gpu:3']\n",
    "\n",
    "# Environment Seeds\n",
    "tf_seed = 2001 # Tensorflow\n",
    "np_seed = 2001 # Numpy\n",
    "py_seed = 2001 # Python\n",
    "\n",
    "# Pre-Load Determiner.\n",
    "preload = False\n",
    "#   ^^^\n",
    "# - True, the candidates are loaded and stored. \n",
    "# - False, the candidates are loaded from jupyter persistent storage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Weights to load when skipping training\n",
    "std_weight_path = '/data/GeometricErrors/tests/BaselineWeights.h5'\n",
    "\n",
    "\n",
    "\"\"\" Point Shifting Annotator Configuration \"\"\"\n",
    "\n",
    "# number of points to generate on BOTH sides of each source point\n",
    "pairs = 20\n",
    "\n",
    "# Interval at which shifting vertices are sampled in meters\n",
    "interval = 10\n",
    "\n",
    "# Offset distance between coordinate points as they are generated on either side of the source. (meters)\n",
    "off_dist = 1.5\n",
    "\n",
    "# Minimum Valid Proability over map\n",
    "min_probability = 1e-06\n",
    "\n",
    "# Weight consideration candidate0 line distance between candidate points\n",
    "length_weight_value = 0\n",
    "\n",
    "# Buffer in meters to apply to candidates\n",
    "weight_buffer = 15\n",
    "\n",
    "# Option to normalize over K^2 (True) or by K (False).\n",
    "normalize_full = False\n",
    "\n",
    "\n",
    "\"\"\" UNET Config \"\"\"\n",
    "\n",
    "# Base learning rate to use in training. Reccomend 0.01\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" EM Iteration Config \"\"\"\n",
    "\n",
    "# Initial jump-start predicted class map path\n",
    "seg_class_map_fp = '/data/pmap.tif'\n",
    "\n",
    "# Number of EM iterations\n",
    "em_target = 6\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "import lib.K_Tools as kt \n",
    "lr_schedule = kt.Oscillate_LR\n",
    "\n",
    "# Optionally Enable notes\n",
    "prompt_notes = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded tensors.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "# Std Imports\n",
    "import os, sys, time, random, csv\n",
    "from datetime import datetime as dt \n",
    "\n",
    "# Module Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely.geometry as shp\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Lib imports\n",
    "import lib.Doc_Tools as doc\n",
    "import lib.GeoTools as gt\n",
    "import lib.Tiling as tile\n",
    "import lib.K_Tools as kt\n",
    "from lib.Annotators import *\n",
    "\n",
    "\"\"\" Static Configuration \"\"\"\n",
    "# Batch size for training\n",
    "batch_size = 32\n",
    "# Early Stopping Patience\n",
    "es_patience = 20\n",
    "# Reduce Learning Rate on Plateau: Multiplier to reduce LR by upon plateau\n",
    "rlop_factor = 0.5\n",
    "# Adam Optimizer config\n",
    "adam_epsilon = 1e-8\n",
    "adam_decay = 1e-5\n",
    "# Buffer in meters applied to annotations when comparing iou\n",
    "iou_buffer = 15\n",
    "\n",
    "# Seed environment\n",
    "random.seed(py_seed)\n",
    "tf.random.set_seed(tf_seed)\n",
    "np.random.seed(np_seed)\n",
    "\n",
    "# Prompt for notes on test\n",
    "if prompt_notes:\n",
    "    test_notes = input(\"Enter Notes for test: \")\n",
    "else:\n",
    "    test_notes = 'Notes have been disabled.'\n",
    "    \n",
    "### Create Folder for test documentation\n",
    "test_idx, test_dir = doc.InitTest(out_root_dir,\n",
    "                                  em_target=em_target,\n",
    "                                  LR=learning_rate,\n",
    "                                  notes=test_notes,\n",
    "                                  pairs=pairs,\n",
    "                                  interval=interval,\n",
    "                                  off_distance=off_dist,\n",
    "                                  min_p=min_probability,\n",
    "                                  L=length_weight_value)\n",
    "\n",
    "### Loading Source\n",
    "train_raster = rio.open('{}/train_raster.tif'.format(source_path))\n",
    "test_raster = rio.open('{}/test_raster.tif'.format(source_path))\n",
    "#template_raster = rio.open('{}/template_map.tif'.format(source_path))\n",
    "\n",
    "gt_labels = gpd.read_file('{}/refined_lines_smaller_area.shp'.format(source_path))\n",
    "imp_labels = gpd.read_file('{}/imperfect_lines_smaller_dataset.shp'.format(source_path))\n",
    "\n",
    "### Loading Tensors\n",
    "X_train = np.load('{}/X_train.npy'.format(data_path))\n",
    "Y_train = np.load('{}/Y_train.npy'.format(data_path))\n",
    "\n",
    "X_val = np.load('{}/X_val.npy'.format(data_path))\n",
    "Y_val = np.load('{}/Y_val.npy'.format(data_path))\n",
    "\n",
    "X_test = np.load('{}/X_test.npy'.format(data_path))\n",
    "Y_test = np.load('{}/Y_test.npy'.format(data_path))\n",
    "\n",
    "\n",
    "### Loading tile offsets\n",
    "train_offsets_fp = '{}/train_offsets.csv'.format(data_path)\n",
    "val_offsets_fp = '{}/val_offsets.csv'.format(data_path)\n",
    "print(\"Successfully loaded tensors.\")\n",
    "\n",
    "# Convert All CRS's\n",
    "imp_labels = imp_labels.to_crs(train_raster.crs)\n",
    "gt_labels = gt_labels.to_crs(imp_labels.crs)\n",
    "source_annotation = imp_labels\n",
    "\n",
    "### Evaluate original Shapefile Precision\n",
    "source_iou = gt.gdf_iou(gt_labels, imp_labels, iou_buffer)\n",
    "prev_iou = source_iou\n",
    "\n",
    "### Create A distributed training strategy for GPU training\n",
    "if use_gpu:\n",
    "    mirror_strategy = tf.distribute.MirroredStrategy(devices=gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Baseline Model Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing UNET model...\n",
      "INFO:tensorflow:batch_all_reduce: 1 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 1 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tried to convert 'mean' to a tensor and failed. Error: Device assignment required for nccl collective ops",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    469\u001b[0m               \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m               preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\u001b[0m in \u001b[0;36m_tensor_conversion_sync_on_read\u001b[0;34m(var, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_tensor_conversion_sync_on_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_var_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\u001b[0m in \u001b[0;36m_dense_var_to_tensor\u001b[0;34m(self, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1060\u001b[0m       return ops.convert_to_tensor(\n\u001b[0;32m-> 1061\u001b[0;31m           self._get(), dtype=dtype, name=name, as_ref=as_ref)\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreplica_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_cross_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\u001b[0m in \u001b[0;36m_get_cross_replica\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m           axis=None)\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, reduce_op, value, axis)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduce_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mreduce_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceOp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, reduce_op, value)\u001b[0m\n\u001b[1;32m   1902\u001b[0m         self.reduce_to(reduce_op, value,\n\u001b[0;32m-> 1903\u001b[0;31m                        device_util.current() or \"/device:CPU:0\"))[0]\n\u001b[0m\u001b[1;32m   1904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mreduce_to\u001b[0;34m(self, reduce_op, value, destinations, experimental_hints)\u001b[0m\n\u001b[1;32m   1930\u001b[0m       \u001b[0mexperimental_hints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperimental_hints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\u001b[0m in \u001b[0;36m_reduce_to\u001b[0;34m(self, reduce_op, value, destinations, experimental_hints)\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0mdestinations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         experimental_hints=experimental_hints)\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, reduce_op, per_replica_value, destinations, experimental_hints)\u001b[0m\n\u001b[1;32m    266\u001b[0m     return self.reduce_implementation(reduce_op, per_replica_value,\n\u001b[0;32m--> 267\u001b[0;31m                                       destinations, experimental_hints)\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py\u001b[0m in \u001b[0;36mreduce_implementation\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_devices_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_replica_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_all_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mper_replica_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py\u001b[0m in \u001b[0;36m_batch_all_reduce\u001b[0;34m(self, reduce_op, per_replica_values)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdense_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mdense_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_batch_all_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py\u001b[0m in \u001b[0;36m_do_batch_all_reduce\u001b[0;34m(self, reduce_op, dense_values)\u001b[0m\n\u001b[1;32m    710\u001b[0m       reduced = cross_device_utils.aggregate_gradients_using_nccl(\n\u001b[0;32m--> 711\u001b[0;31m           device_grad_packs)\n\u001b[0m\u001b[1;32m    712\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_utils.py\u001b[0m in \u001b[0;36maggregate_gradients_using_nccl\u001b[0;34m(replica_grads)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0msingle_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msingle_g_and_v\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0magg_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnccl_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     agg_all_g_and_v.append(\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/ops/nccl_ops.py\u001b[0m in \u001b[0;36mall_sum\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_apply_all_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/ops/nccl_ops.py\u001b[0m in \u001b[0;36m_apply_all_reduce\u001b[0;34m(reduction, tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_all_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/ops/nccl_ops.py\u001b[0m in \u001b[0;36m_all_reduce\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0m_check_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/ops/nccl_ops.py\u001b[0m in \u001b[0;36m_check_device\u001b[0;34m(tensor, expected)\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonical_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Device assignment required for nccl collective ops'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Device assignment required for nccl collective ops",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    483\u001b[0m             observed = ops.convert_to_tensor(\n\u001b[0;32m--> 484\u001b[0;31m                 values, as_ref=input_arg.is_ref).dtype.name\n\u001b[0m\u001b[1;32m    485\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\u001b[0m in \u001b[0;36m_tensor_conversion_sync_on_read\u001b[0;34m(var, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_tensor_conversion_sync_on_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_var_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\u001b[0m in \u001b[0;36m_dense_var_to_tensor\u001b[0;34m(self, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1060\u001b[0m       return ops.convert_to_tensor(\n\u001b[0;32m-> 1061\u001b[0;31m           self._get(), dtype=dtype, name=name, as_ref=as_ref)\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreplica_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_cross_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\u001b[0m in \u001b[0;36m_get_cross_replica\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m           axis=None)\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, reduce_op, value, axis)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduce_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mreduce_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceOp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, reduce_op, value)\u001b[0m\n\u001b[1;32m   1902\u001b[0m         self.reduce_to(reduce_op, value,\n\u001b[0;32m-> 1903\u001b[0;31m                        device_util.current() or \"/device:CPU:0\"))[0]\n\u001b[0m\u001b[1;32m   1904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mreduce_to\u001b[0;34m(self, reduce_op, value, destinations, experimental_hints)\u001b[0m\n\u001b[1;32m   1930\u001b[0m       \u001b[0mexperimental_hints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperimental_hints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\u001b[0m in \u001b[0;36m_reduce_to\u001b[0;34m(self, reduce_op, value, destinations, experimental_hints)\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0mdestinations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         experimental_hints=experimental_hints)\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, reduce_op, per_replica_value, destinations, experimental_hints)\u001b[0m\n\u001b[1;32m    266\u001b[0m     return self.reduce_implementation(reduce_op, per_replica_value,\n\u001b[0;32m--> 267\u001b[0;31m                                       destinations, experimental_hints)\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py\u001b[0m in \u001b[0;36mreduce_implementation\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_devices_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_replica_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_all_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mper_replica_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py\u001b[0m in \u001b[0;36m_batch_all_reduce\u001b[0;34m(self, reduce_op, per_replica_values)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdense_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mdense_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_batch_all_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py\u001b[0m in \u001b[0;36m_do_batch_all_reduce\u001b[0;34m(self, reduce_op, dense_values)\u001b[0m\n\u001b[1;32m    710\u001b[0m       reduced = cross_device_utils.aggregate_gradients_using_nccl(\n\u001b[0;32m--> 711\u001b[0;31m           device_grad_packs)\n\u001b[0m\u001b[1;32m    712\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_utils.py\u001b[0m in \u001b[0;36maggregate_gradients_using_nccl\u001b[0;34m(replica_grads)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0msingle_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msingle_g_and_v\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0magg_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnccl_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     agg_all_g_and_v.append(\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/ops/nccl_ops.py\u001b[0m in \u001b[0;36mall_sum\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_apply_all_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/ops/nccl_ops.py\u001b[0m in \u001b[0;36m_apply_all_reduce\u001b[0;34m(reduction, tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_all_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/ops/nccl_ops.py\u001b[0m in \u001b[0;36m_all_reduce\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0m_check_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/ops/nccl_ops.py\u001b[0m in \u001b[0;36m_check_device\u001b[0;34m(tensor, expected)\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonical_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Device assignment required for nccl collective ops'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Device assignment required for nccl collective ops",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-923ea8f4f299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmirror_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Select and Build Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGet_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UNET'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam_epsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdice_coef_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GeometricErrors/lib/K_Tools.py\u001b[0m in \u001b[0;36mGet_Model\u001b[0;34m(key, dropout)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'UNET'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m## Unet selected, no parameters required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNET_7_224\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m'SegNet'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GeometricErrors/lib/models/UNET.py\u001b[0m in \u001b[0;36mUNET_7_224\u001b[0;34m(dropout_val, std_init)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mconv_224\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdouble_conv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mpool_112\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GeometricErrors/lib/models/UNET.py\u001b[0m in \u001b[0;36mdouble_conv_layer\u001b[0;34m(x, size, std_init, dropout, batch_norm)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch_norm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    921\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fused_batch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;31m# Currently never reaches here since fused_batch_norm does not support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     output, mean, variance = tf_utils.smart_cond(training, train_op,\n\u001b[0;32m--> 604\u001b[0;31m                                                  _fused_batch_norm_inference)\n\u001b[0m\u001b[1;32m    605\u001b[0m     \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_add_or_remove_bessels_correction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     63\u001b[0m         pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[1;32m     64\u001b[0m   return smart_module.smart_cond(\n\u001b[0;32m---> 65\u001b[0;31m       pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\n\u001b[0;32m---> 59\u001b[0;31m                                  name=name)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mcond\u001b[0;34m(pred, true_fn, false_fn, strict, name, fn1, fn2)\u001b[0m\n\u001b[1;32m   1175\u001b[0m   if (util.EnableControlFlowV2(ops.get_default_graph()) and\n\u001b[1;32m   1176\u001b[0m       not context.executing_eagerly()):\n\u001b[0;32m-> 1177\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcond_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m   \u001b[0;31m# We needed to make true_fn/false_fn keyword arguments for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36mcond_v2\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     82\u001b[0m             true_name, collections=ops.get_default_graph()._collections),  # pylint: disable=protected-access\n\u001b[1;32m     83\u001b[0m         \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         op_return_value=pred)\n\u001b[0m\u001b[1;32m     85\u001b[0m     false_graph = func_graph_module.func_graph_from_py_func(\n\u001b[1;32m     86\u001b[0m         \u001b[0mfalse_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m    577\u001b[0m           \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m           exponential_avg_factor=exponential_avg_factor)\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fused_batch_norm_training_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mfused_batch_norm\u001b[0;34m(x, scale, offset, mean, variance, epsilon, data_format, is_training, name, exponential_avg_factor)\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1544\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   1545\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mfused_batch_norm_v3\u001b[0;34m(x, scale, offset, mean, variance, epsilon, exponential_avg_factor, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   4277\u001b[0m                             \u001b[0mexponential_avg_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexponential_avg_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4278\u001b[0m                             \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4279\u001b[0;31m                             name=name)\n\u001b[0m\u001b[1;32m   4280\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4281\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/geo_errors/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    486\u001b[0m             raise ValueError(\n\u001b[1;32m    487\u001b[0m                 \u001b[0;34m\"Tried to convert '%s' to a tensor and failed. Error: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m                 (input_name, err))\n\u001b[0m\u001b[1;32m    489\u001b[0m           prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[1;32m    490\u001b[0m                     (input_name, op_type_name, observed))\n",
      "\u001b[0;31mValueError\u001b[0m: Tried to convert 'mean' to a tensor and failed. Error: Device assignment required for nccl collective ops"
     ]
    }
   ],
   "source": [
    "### Baseline Model Training\n",
    "\n",
    "# Get timer for baseline training\n",
    "base_start = time.perf_counter()\n",
    "\n",
    "# Prepare Baseline Folder\n",
    "base_folder = os.path.join(test_dir, 'baseline')\n",
    "if not os.path.exists(base_folder): os.mkdir(base_folder)\n",
    "\n",
    "# Prepare tensorboard folder\n",
    "base_tb_dir = os.path.join(base_folder, 'tensorboard')\n",
    "    \n",
    "# Prepare Callbacks, including weight output\n",
    "base_callbacks = kt.SetCallbacks(weights_out=base_folder+'/BaselineWeights.h5', es_patience = es_patience, rlop_factor = rlop_factor, tensorboard_path=base_tb_dir)\n",
    "metrics = [kt.dice_coef,'accuracy', kt.f1_score]\n",
    "    \n",
    "if use_gpu:\n",
    "    with mirror_strategy.scope():\n",
    "        # Select and Build Model\n",
    "        model = kt.Get_Model('UNET')\n",
    "        model.compile(optimizer=Adam(lr=learning_rate, epsilon=adam_epsilon, decay=adam_decay), loss = kt.dice_coef_loss, metrics=metrics)\n",
    "\n",
    "        # Train Model\n",
    "        baseline_results = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), shuffle=True, batch_size=batch_size, epochs=epochs, callbacks=base_callbacks)\n",
    "else:\n",
    "    # Select and Build Model\n",
    "    model = kt.Get_Model('UNET')\n",
    "    model.compile(optimizer=Adam(lr=learning_rate, epsilon=adam_epsilon, decay=adam_decay), loss = kt.dice_coef_loss, metrics=metrics)\n",
    "\n",
    "    # Train Model\n",
    "    baseline_results = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), shuffle=True, batch_size=batch_size, epochs=epochs, callbacks=base_callbacks)\n",
    "\n",
    "# Save History plot and csv\n",
    "doc.plot_history(baseline_results, test_dir=base_folder, config_idx='base')\n",
    "\n",
    "epochs_used = len(baseline_results.history['accuracy'])\n",
    "\n",
    "    \n",
    "\"\"\" Evaluate Baseline Model Preformance \"\"\"\n",
    "print(\"Baseline Preformance:\")\n",
    "train_rpt = kt.ModelReport(X_train, Y_train, model, 'Training')\n",
    "val_rpt = kt.ModelReport(X_val, Y_val, model, 'Validation')\n",
    "test_rpt = kt.ModelReport(X_test, Y_test, model, 'Testing')\n",
    "\n",
    "print(dt.now().strftime('\\n\\n%a at %I:%M:%S%p'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: EM Iteration\n",
    "\n",
    "`Warning`: By proceeding, the EM Iteration will use the model configured above with the parameters already set. Tune the baseline model above as many times as needed before proceeding.\n",
    "\n",
    "#### Section 1: Configure EM Test with Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare annotator\n",
    "annotator = Dynamic_Preloading_Annotator(pairs=pairs,                   # Number of pairs to be linearly generated on either side of the source point \n",
    "                                         off_dist=off_dist,             # Interval distance that each new pair is generated\n",
    "                                         interval=interval,             # Inteval at which new candidate groups are generated\n",
    "                                         min_p=min_probability,         # Minimum valid probability from class map\n",
    "                                         L=length_weight_value,         # Weight for line distance\n",
    "                                         weight_buffer=weight_buffer,   # Buffer to apply to candidates when weighting\n",
    "                                         normalize_full=normalize_full, # Optionally normalize by K^2 instead of K\n",
    "                                         )\n",
    "\n",
    "#copy_anno = annotator\n",
    "\n",
    "# Preload Candidate Data \n",
    "if preload:\n",
    "    print(\"PRELOADING\")\n",
    "    initial_pmap = rio.open(seg_class_map_fp)\n",
    "    all_data = annotator.preload_candidates(source_annotation, initial_pmap)\n",
    "    %store all_data\n",
    "    #%store annotator\n",
    "else:\n",
    "    %store -r all_data\n",
    "    #%store -r annotator\n",
    "    \n",
    "    # Check the annotators for equivalent configuration\n",
    "    \"\"\"bad_keys = []\n",
    "    for key in ['pairs', 'off_dist', 'interval', 'min_p', 'L', 'weight_buffer', 'normalize_full']:\n",
    "        if (getattr(annotator, key) != getattr(copy_anno, key)):\n",
    "            bad_keys.append(key)\n",
    "    if len(bad_keys) > 0:\n",
    "        raise RuntimeError(\"Annotators have {} incompatible keys: {}\\nCandidates must be regenerated. Configure this in the first block.\".format(len(bad_keys),bad_keys))\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Loaded candidate_data\")\n",
    "    \n",
    "    \n",
    "# Prepare performance storage\n",
    "em_dict = {\n",
    "    'Name': ['Base'],\n",
    "    'Test_Data': [test_rpt],\n",
    "    'Train_Data': [train_rpt],\n",
    "    'Val_Data': [val_rpt],\n",
    "    'Line_IoU': [np.round((source_iou*100), 2)],\n",
    "    'Epochs': [epochs_used],\n",
    "    'LR': [learning_rate],\n",
    "    'Training_Time': ['{:.3f} s'.format(time.perf_counter() - base_start)],\n",
    "    'Update_Time': ['NA'],\n",
    "}\n",
    "\n",
    "# Prepare EM iterator index.\n",
    "EM_iterator = 0\n",
    "\n",
    "# Prepare storage for top F1 and IoU\n",
    "top_f1, top_f1_idx = 0, 0\n",
    "top_iou, top_iou_idx = 0, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run EM Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Beginning Iteration, Target steps:\", em_target)\n",
    "if EM_iterator > 0: print(\"Current Step:\", EM_iterator)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Begin Iteration -------------------------------------------------------------\n",
    "while EM_iterator < em_target: # Note: using while loop to enable additional runs after test conclusion\n",
    "    \n",
    "    # 0. Initialization\n",
    "    # ---------------------------\n",
    "    step_start = time.perf_counter()\n",
    "\n",
    "    # Create folder for em step\n",
    "    emfolder = os.path.join(test_dir, 'Step_{:02}'.format(EM_iterator))\n",
    "    if not os.path.exists(emfolder): os.mkdir(emfolder)\n",
    "\n",
    "    print(\"\\nEM Step {:02} begun.\".format(EM_iterator))\n",
    "    print(dt.now().strftime('%a at %I:%M:%S%p'))\n",
    "\n",
    "    \n",
    "    # 1. Update Annotations\n",
    "    # ---------------------------\n",
    "\n",
    "    # 1.1 Get probability output map from last model \n",
    "    if EM_iterator == 0:\n",
    "        # If on the first EM step, we load from previous data\n",
    "        class_map_start = time.perf_counter()\n",
    "        pmap_fp = os.path.join(emfolder, 'pmap_baseline.tif')\n",
    "        kt.Get_Pmap(train_raster, model, pmap_fp)\n",
    "        predicted_class_map = rio.open(seg_class_map_fp)\n",
    "    else:\n",
    "        # On every other EM step we take the pmap from the last model \n",
    "        class_map_start = time.perf_counter()\n",
    "        pmap_fp = os.path.join(emfolder, 'pmap_{:02}.tif'.format(EM_iterator))\n",
    "        predicted_class_map = kt.Get_Pmap(train_raster, model, pmap_fp)\n",
    "        \n",
    "    print(\"\\nGenerated Predicted Class and Intermediate Feature Maps from previous model. (Step {:02})\".format(EM_iterator))\n",
    "    print(dt.now().strftime('%a at %I:%M:%S%p'))\n",
    "    \n",
    "        \n",
    "    # 1.2 Update annotation\n",
    "    update_start = time.perf_counter()\n",
    "    annotation_fp = os.path.join(emfolder, 'annotation_{:02}.shp'.format(EM_iterator))\n",
    "    new_annotation = annotator.update_gdf_from_preload(all_data, class_map=predicted_class_map, out_path=annotation_fp) \n",
    "\n",
    "    # 1.3 Generate and save all considered point groups\n",
    "    candidate_start = time.perf_counter()\n",
    "    candidate_fp = os.path.join(emfolder, 'candidates_{:02}.shp'.format(EM_iterator))\n",
    "    annotator.get_candidates(source_annotation, class_map=predicted_class_map, out_path=candidate_fp)\n",
    "    \n",
    "    # 1.5 Save iou for this annotation.\n",
    "    iou_start = time.perf_counter()\n",
    "    anno_iou = gt.gdf_iou(gt_labels, new_annotation, iou_buffer)\n",
    "\n",
    "\n",
    "    print(\"\\nCreated New Annotation. (Step {:02})\".format(EM_iterator))\n",
    "    print(dt.now().strftime('%a at %I:%M:%S%p'))\n",
    "\n",
    "\n",
    "    # 2. Create new Label Tensors\n",
    "    # ---------------------------\n",
    "    \n",
    "    # 2.1 Rasterize New Labels\n",
    "    rasterize_start = time.perf_counter()\n",
    "    buff_anno = gt.gdf_buffer(new_annotation, buff_dist=2, flatten=True)\n",
    "    anno_raster_fp = os.path.join(emfolder, 'rasterized_annotation_{:02}.tif'.format(EM_iterator))\n",
    "    anno_raster = gt.GDF_Rasterize(buff_anno, train_raster, out_path=anno_raster_fp)\n",
    "    \n",
    "    # 2.2 Read Y_train, Y_val\n",
    "    sample_tensor_start = time.perf_counter()\n",
    "    Y_train = tile.ResampleTiles(anno_raster, train_offsets_fp)\n",
    "    Y_val = tile.ResampleTiles(anno_raster, val_offsets_fp)\n",
    "    \n",
    "    # 2.2 Augment Y_train, Y_val\n",
    "    Y_train = tile.AugmentImages(Y_train)\n",
    "    Y_val = tile.AugmentImages(Y_val)\n",
    "    \n",
    "    print(\"\\nCreated Y_train {} and Y_val {}. (Step {:02})\".format(Y_train.shape, Y_val.shape, EM_iterator))\n",
    "    print(dt.now().strftime('%a at %I:%M:%S%p'))\n",
    "    \n",
    "    \n",
    "    # 3. Re-Train U-Net \n",
    "    # ---------------------------\n",
    "    \n",
    "    # 3.1 Load Callbacks \n",
    "    weight_path = os.path.join(emfolder, 'unet_weights_{:02}.h5'.format(EM_iterator))\n",
    "    tensorboard_path = os.path.join(emfolder, 'tensorboard_{:02}'.format(EM_iterator))\n",
    "    callbacks =  kt.SetCallbacks(weights_out = weight_path, es_patience = es_patience, rlop_factor = rlop_factor, tensorboard_path = tensorboard_path)#kt.SetCallbacks(emfolder + '/{}_{:02}.h5'.format(model_key, EM_iterator))\n",
    "    \n",
    "    # 3.2 Determine Learning Rate from internal oscilator\n",
    "    EM_learning_rate = lr_schedule(learning_rate, EM_iterator, em_target)\n",
    "    print(\"\\nNew Learning Rate: {:.6f}. (Step {:02})\".format(EM_learning_rate, EM_iterator))\n",
    "    \n",
    "    # 3.3 Train Model\n",
    "    \n",
    "    model_training_start = time.perf_counter()\n",
    "    if use_gpu:\n",
    "        with mirror_strategy.scope():\n",
    "            model = kt.Get_Model('UNET')\n",
    "            model.compile(optimizer=Adam(lr=EM_learning_rate, epsilon=adam_epsilon, decay=adam_decay), loss=kt.dice_coef_loss, metrics=metrics)\n",
    "            print(\"\\nTraining model...\")\n",
    "            training_history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), shuffle=True, batch_size=batch_size, epochs=epochs, callbacks=callbacks, verbose=0)\n",
    "    else:\n",
    "        model = kt.Get_Model('UNET')\n",
    "        model.compile(optimizer=Adam(lr=EM_learning_rate, epsilon=adam_epsilon, decay=adam_decay), loss=kt.dice_coef_loss, metrics=metrics)\n",
    "        print(\"\\nTraining model...\")\n",
    "        training_history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), shuffle=True, batch_size=batch_size, epochs=epochs, callbacks=callbacks, verbose=0)\n",
    "    \n",
    "     \n",
    "    \n",
    "    print(\"Completed model Training. (Step {:02})\".format(EM_iterator))\n",
    "    print(dt.now().strftime('%a at %I:%M:%S%p'))\n",
    "    \n",
    "\n",
    "\n",
    "    # 4. Evaluate Model\n",
    "    # ---------------------------\n",
    "    \n",
    "    evaluate_start = time.perf_counter()\n",
    "\n",
    "    # Save History Plots and CSV\n",
    "    doc.plot_history(training_history, test_dir=emfolder, config_idx = EM_iterator)\n",
    "    \n",
    "    hist_markdown_fp = os.path.join(emfolder, 'history_{:02}.md'.format(EM_iterator))\n",
    "    hist_md = open(hist_markdown_fp, 'w+')\n",
    "    \n",
    "    # Each of the reports below contains a dict with 'F1_Score', 'Dice' and their original floats\n",
    "    test_rpt = kt.ModelReport(X_test, Y_test, model, \"Testing\", index=(EM_iterator+1), report_md=hist_md)\n",
    "    train_rpt = kt.ModelReport(X_train, Y_train, model, \"Training\", index=(EM_iterator+1), report_md=hist_md)\n",
    "    val_rpt = kt.ModelReport(X_val, Y_val, model, \"Validation\", index=(EM_iterator+1), report_md=hist_md)\n",
    "    \n",
    "    hist_md.close()\n",
    "    \n",
    "    evaluate_end = time.perf_counter()\n",
    "    \n",
    "    # Update reference dict\n",
    "    em_dict['Name'].append('Step {:02}'.format(EM_iterator))\n",
    "    em_dict['Line_IoU'].append(np.round((anno_iou*100), 2))\n",
    "    \n",
    "    em_dict['Test_Data'].append(test_rpt)\n",
    "    em_dict['Train_Data'].append(train_rpt)\n",
    "    em_dict['Val_Data'].append(val_rpt)\n",
    "        \n",
    "    em_dict['Epochs'].append(len(training_history.history['accuracy']))\n",
    "    em_dict['LR'].append(EM_learning_rate)\n",
    "    em_dict['Update_Time'].append('{:.3f} s'.format(candidate_start - update_start))\n",
    "    em_dict['Training_Time'].append('{:.3f} s'.format(evaluate_start - model_training_start))\n",
    "    \n",
    "    \n",
    "    # Update top values\n",
    "    ### f1 score\n",
    "    if test_rpt['F1_Score'] > top_f1:\n",
    "        top_f1 = test_rpt['F1_Score']\n",
    "        top_f1_idx = EM_iterator\n",
    "        print(\"\\nNew Top F1: {:.2f}\".format(top_f1*100))\n",
    "    ### Annotation IoU\n",
    "    if np.round((anno_iou*100), 2) > top_iou:\n",
    "        top_iou = np.round((anno_iou*100), 2)\n",
    "        top_iou_idx = EM_iterator\n",
    "        print(\"\\nNew Top IoU: {:.2f}\".format(top_iou))\n",
    "    \n",
    "    # Print step data\n",
    "    print(\"\\nEM Step ({:02}) Complete on {}\".format(EM_iterator, dt.now().strftime('%a at %I:%M:%S%p')))\n",
    "    print('- Annotation IoU:     {:.2f}'.format(anno_iou*100))\n",
    "    print('\\t- Source Improvement: {:+.2f}'.format((anno_iou-source_iou)*100))\n",
    "    print('\\t- Step Improvement:   {:+.2f}'.format((anno_iou-prev_iou)*100))\n",
    "    print()\n",
    "    print(\"------ Times ---------------------\")\n",
    "    print(\"- Class Map Generation: {:.3f} s\".format(update_start - class_map_start))\n",
    "    print(\"- Annotation Update:    {:.3f} s\".format(candidate_start - update_start))\n",
    "    print(\"- Model Training:       {:.3f} s\".format(evaluate_start - model_training_start))\n",
    "    print(\"- Model Evaluation:     {:.3f} s\".format(evaluate_end - evaluate_start))\n",
    "    print(\"----------------------------------\\n\\n\")\n",
    "    \n",
    "    # Increase iterator and save previous precision for step_delta\n",
    "    EM_iterator += 1\n",
    "    prev_iou = anno_iou\n",
    " \n",
    "# Increase EM target for optional subsequent runs\n",
    "em_target += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean data for easy indexing\n",
    "\n",
    "## Converts each of the report lists into a dict of lists for each value\n",
    "model_dict = {'Test_Data': {}, 'Train_Data': {}, 'Val_Data': {}}\n",
    "for em_key in model_dict.keys():\n",
    "    for report in em_dict[em_key]:\n",
    "        for rpt_key, rpt_value in [(key, item) for key, item in report.items()]:\n",
    "            if rpt_key not in model_dict[em_key].keys():\n",
    "                model_dict[em_key].update({rpt_key: np.array([rpt_value])})\n",
    "            else:\n",
    "                model_dict[em_key][rpt_key] = np.append(model_dict[em_key][rpt_key], report[rpt_key])\n",
    "\n",
    "\n",
    "# Create Figure for Plots\n",
    "fig, axs = plt.subplots(2, 2, sharex=True, figsize=(16,10))\n",
    "\n",
    "## Plot Testing F1\n",
    "doc.plot_axis(ax=axs[0,0], \n",
    "              data=model_dict['Test_Data']['F1_Score']*100, \n",
    "              name='Testing F1', \n",
    "              color_char='r', \n",
    "              symbol_char='s', \n",
    "              y_off=2,\n",
    "              label_delta=False)\n",
    "\n",
    "## Plot Training F1\n",
    "doc.plot_axis(ax=axs[0,1], \n",
    "              data=model_dict['Train_Data']['F1_Score']*100, \n",
    "              name='Training F1', \n",
    "              color_char='g', \n",
    "              symbol_char='^', \n",
    "              y_off=2,\n",
    "              label_delta=False)\n",
    "\n",
    "## Plot Validation F1\n",
    "doc.plot_axis(ax=axs[1,0], \n",
    "              data=model_dict['Val_Data']['F1_Score']*100, \n",
    "              name='Validation F1', \n",
    "              color_char='c', \n",
    "              symbol_char='^', \n",
    "              y_off=2,\n",
    "              label_delta=False)\n",
    "\n",
    "## Plot Annotation IoU\n",
    "doc.plot_axis(ax=axs[1,1], \n",
    "              data=em_dict['Line_IoU'], \n",
    "              name='Line IoU', \n",
    "              color_char='m', \n",
    "              x_label='EM Step',\n",
    "              label_delta=False)\n",
    "    \n",
    "## Title and show, and save figure\n",
    "fig.suptitle(\"EM Test {:02}\".format(test_idx))\n",
    "fig_path = os.path.join(test_dir, 'test_{:02}_plot.png'.format(test_idx))\n",
    "fig.savefig(fig_path)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to Markdown\n",
    "\n",
    "Saves:\n",
    "- Annotator Config\n",
    "- Model Config\n",
    "- EM preformance table\n",
    "- Model preformance table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Write test data to markdown\n",
    "markdown_fp = os.path.join(test_dir, 'em_test_info_{:02}.md'.format(test_idx))\n",
    "md = open(markdown_fp, 'w+')\n",
    "\n",
    "# Header / Notes\n",
    "md.write(\"# EM Test {:02}\\n\\n\".format(test_idx))\n",
    "\n",
    "if (len(test_notes) > 0):\n",
    "    md.write(\"### Notes: \\n\")\n",
    "    md.write(test_notes)\n",
    "\n",
    "md.write(dt.now().strftime('\\n%a at %I:%M:%S%p'))\n",
    "    \n",
    "md.write(\"\\n\\n---\\n\\n\")\n",
    "\n",
    "# prepare peak values\n",
    "top_f1 = np.round((top_f1*100), 3)\n",
    "source_f1 = np.round((em_dict['Test_Data'][0]['F1_Score']*100), 3)\n",
    "source_iou = np.round(source_iou*100, 2)\n",
    "\n",
    "# Note at top the peak values \n",
    "md.write(\"### Top Values:\\n\\n\")\n",
    "md.write(\" - Testing F1 Score: **{:.2f}** (`{:+.2f}`) - Step {:02}\\n\".format(top_f1, (top_f1-source_f1), top_f1_idx))\n",
    "md.write(\" - Annotation IoU: **{:.2f}** (`{:+.2f}`) - Step {:02}\\n\\n\\n\".format(top_iou, (top_iou-source_iou), top_iou_idx))\n",
    "\n",
    "\n",
    "\n",
    "# Test Configuration Section\n",
    "md.write(\"## **Test Config**\\n\\n\")\n",
    "\n",
    "md.write(\"### Seeds:\\n\")\n",
    "md.write(\"`Python`: {}, `Numpy`: {}, `Tensorflow`: {}\\n\\n\".format(py_seed, np_seed, tf_seed))\n",
    "\n",
    "md.write(\"### Annotator:\\n\")\n",
    "for key, item in annotator.__dict__.items():\n",
    "    \n",
    "    # Skip gross keys\n",
    "    if key in ['verbosity', 'crop_window', 'class_map', 'kwargs_key', 'crs']:\n",
    "        continue\n",
    "    \n",
    "    # Write key and item\n",
    "    md.write(\"- `{}`: {}\\n\".format(key, item))\n",
    "        \n",
    "md.write(\"\\n\\n\")\n",
    "\n",
    "md.write(\"### Model:\\n\")\n",
    "md.write(\"- **Training**:\\n\")\n",
    "md.write(\"  - `Batch Size`: {}\\n\".format(batch_size))\n",
    "md.write(\"  - `Learning Rate`: {}\\n\".format(learning_rate))\n",
    "md.write(\"  - `Epochs`: {}\\n\".format(epochs))\n",
    "md.write(\"- **Callbacks**:\\n\")\n",
    "md.write(\"  - `ES Patience`: {}\\n\".format(es_patience))\n",
    "md.write(\"  - `RLOP Factor`: {}\\n\".format(rlop_factor))\n",
    "md.write(\"  - `Adam Epsilon`: {}\\n\".format(adam_epsilon))\n",
    "md.write(\"  - `Adam Decay`: {}\\n\".format(adam_decay))\n",
    "\n",
    "md.write(\"\\n\\n\")\n",
    "\n",
    "md.write(\"### Other:\\n\")\n",
    "md.write(\"- EM Steps: {}\\n\".format(em_target))\n",
    "md.write(\"- ClassMap: '{}'\\n\".format(seg_class_map_fp))\n",
    "\n",
    "\n",
    "md.write(\"\\n---\\n\\n\")\n",
    "\n",
    "\n",
    "# Results Section\n",
    "\n",
    "md.write(\"## **Results**:\\n\\n\")\n",
    "\n",
    "md.write(\"### EM Iteration:\\n\\n\")\n",
    "\n",
    "md.write(\"Step | Anno IoU | F1 | Epochs | LR | Train | Update\\n\")\n",
    "md.write(\"---- | -------- | -- | ------ | -- | ----- | ------\\n\")\n",
    "for idx in range(em_target):\n",
    "    if idx == 0:\n",
    "        md.write(\"{} | {} | {} | {} | {} | {} | {}\\n\".format(em_dict['Name'][idx], em_dict['Line_IoU'][idx], em_dict['Test_Data'][idx]['F1_Score'], em_dict['Epochs'][idx], em_dict['LR'][idx], em_dict['Training_Time'][idx], em_dict['Update_Time'][idx]))\n",
    "    else:\n",
    "        md.write(\"{} | {} (`{:+.2f}`) | {} | {} | {} | {} | {}\\n\".format(em_dict['Name'][idx], em_dict['Line_IoU'][idx], (em_dict['Line_IoU'][idx] - em_dict['Line_IoU'][0]), em_dict['Test_Data'][idx]['F1_Score'], em_dict['Epochs'][idx], em_dict['LR'][idx], em_dict['Training_Time'][idx], em_dict['Update_Time'][idx]))\n",
    "\n",
    "md.write(\"\\n\\n</br>\\n\\n\")\n",
    "    \n",
    "md.write(\"### Model Performance:\\n\\n\")\n",
    "md.write(\"Step | Test F1 | Test (FP, FN) | Train F1 | Train (FP, FN) | Val F1 | Val (FP, FN) | LR \\n\")\n",
    "md.write(\"---- | ------- | ------------- | -------- | -------------- | ------ | ------------ | -- \\n\")\n",
    "for idx in range(em_target):\n",
    "    # Create a string to hold this row's data\n",
    "    row_string = \"{} | \".format(em_dict['Name'][idx])\n",
    "    \n",
    "    # Iterate over F1 types\n",
    "    for key in ['Test_Data', 'Train_Data', 'Val_Data']:\n",
    "        \n",
    "        if idx == 0:\n",
    "            row_string += \"{:.2f} | ({:.2e}, {:.2e}) | \".format(em_dict[key][idx]['F1_Score']*100, em_dict[key][idx]['False_Positives'], em_dict[key][idx]['False_Negatives'])\n",
    "        else:\n",
    "            row_string += \"{:.2f} (`{:+.2f}`) | ({:.2e}, {:.2e}) | \".format(em_dict[key][idx]['F1_Score']*100, (em_dict[key][idx]['F1_Score']*100 - em_dict[key][0]['F1_Score']*100), em_dict[key][idx]['False_Positives'], em_dict[key][idx]['False_Negatives'])\n",
    "      \n",
    "    # After all sets have been added, append learning rate and newline\n",
    "    row_string += \"{}\\n\".format(em_dict['LR'][idx])\n",
    "    md.write(row_string)\n",
    "\n",
    "\n",
    "# Close Markdown\n",
    "md.close()\n",
    "\n",
    "print(\"Config and results written to markdown.\")\n",
    "print(\"Location:\", markdown_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_errors",
   "language": "python",
   "name": "geo_errors"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
