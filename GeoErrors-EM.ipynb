{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GeometricAnnotationErrors - EM Algorithm\n",
    "### July 2021\n",
    "\n",
    "---\n",
    "\n",
    "Used to run the EM algorithm **AFTER** downloading the source data and running `preprocess.py` to create training, validation, and testing tensors.\n",
    "\n",
    "Test results and plots are output to a indexed sub-folder (like 'em_test_00/') under the directory specified by `RESULTS_DIR` in `config.py`. If you do not have a `config.py` with this variable in your root directory, use `setup.py` to initialize your environment. Feel free to set the test output root directory to any folder that does not require root permissions to access. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Load Filepaths \"\"\"\n",
    "from config import INPUT_DATA_DIR, TENSOR_DIR, RESULTS_DIR\n",
    "data_path = TENSOR_DIR\n",
    "source_path = INPUT_DATA_DIR\n",
    "out_root_dir = RESULTS_DIR\n",
    "\n",
    "# Output level - Higher levels include lower outputs\n",
    "# 0: \n",
    "# - info markdown, \n",
    "# - step-wise graph for train/val/test F1, shapefile IoU \n",
    "# 1: \n",
    "# - model plots (each step) \n",
    "# - model weights (each step)\n",
    "# - updated annotation shapefiles\n",
    "# 2+: \n",
    "# - Predicted class map rasters\n",
    "# - annotation rasters, \n",
    "# - model reports in text\n",
    "# - Training history for each step\n",
    "# - Tensorboard for every step\n",
    "OUTPUT_LEVEL = 0\n",
    "\n",
    "import os\n",
    "\n",
    "# Module Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely.geometry as shp\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Lib imports\n",
    "import lib.Doc_Tools as doc\n",
    "import lib.GeoTools as gt\n",
    "import lib.Tiling as tile\n",
    "import lib.K_Tools as kt\n",
    "\n",
    "from lib import *\n",
    "from lib.envtools import gettime, lr_schedule\n",
    "from lib.Doc_Tools import print_s\n",
    "\n",
    "### Model Training Hyperparameters\n",
    "BATCH_SIZE = 32 \n",
    "EPOCHS = 50\n",
    "\n",
    "# Seed environment\n",
    "tf.random.set_seed(2001)\n",
    "np.random.seed(2001)\n",
    "\n",
    "### Create Folder to store test results\n",
    "test_idx, test_dir = doc.InitTest(out_root_dir)\n",
    "\n",
    "# Open Raster Imagery, \n",
    "train_raster = rio.open(os.path.join(source_path, 'train_raster.tif'))\n",
    "\n",
    "# Open Label shapefiles\n",
    "refined_labels  = gpd.read_file(os.path.join(source_path, 'refined_labels.shp'))\n",
    "refined_labels.to_crs(train_raster.crs, inplace=True)\n",
    "\n",
    "imperfect_labels = gpd.read_file(os.path.join(source_path, 'imperfect_labels.shp'))\n",
    "imperfect_labels.to_crs(train_raster.crs, inplace=True)\n",
    "print_s(None, \"Successfully loaded rasters and shapefiles data.\")\n",
    "\n",
    "### Loading Tensors and tile offsets\n",
    "X_train = np.load(os.path.join(data_path, 'X_train.npy'))\n",
    "Y_train = np.load(os.path.join(data_path, 'Y_train.npy'))\n",
    "X_val = np.load(os.path.join(data_path, 'X_val.npy'))\n",
    "Y_val = np.load(os.path.join(data_path, 'Y_val.npy'))\n",
    "X_test = np.load(os.path.join(data_path, 'X_test.npy'))\n",
    "Y_test = np.load(os.path.join(data_path, 'Y_test.npy'))\n",
    "train_offsets_fp = os.path.join(data_path, 'train_offsets.csv')\n",
    "val_offsets_fp = os.path.join(data_path, 'val_offsets.csv')\n",
    "print_s(None, \"Successfully loaded tensors.\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model Training and Evaluation\n",
    "First, the UNet is trained on the source data generated by `preprocess.py`. The results of this training can be found under the 'baseline' sub-folder in the test's output."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prepare Baseline Folder\n",
    "base_folder = os.path.join(test_dir, 'baseline')\n",
    "if not os.path.exists(base_folder): os.mkdir(base_folder)\n",
    "    \n",
    "# Prepare optimizer and callbacks (including weight output)\n",
    "optimizer = Adam(lr=0.1, epsilon=1e-8, decay=1e-5)\n",
    "base_callbacks = kt.SetCallbacks(weights_out=os.path.join(base_folder, 'BaselineWeights.h5'),\n",
    "                                 tensorboard_path=os.path.join(base_folder, 'tensorboard'),\n",
    "                                 output_level=OUTPUT_LEVEL)\n",
    "\n",
    "# Select and Build Model\n",
    "model = kt.Get_Model('UNET')\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss = kt.dice_coef_loss, \n",
    "              metrics=[kt.dice_coef,'accuracy', kt.f1_score])\n",
    "\n",
    "# Train Model\n",
    "baseline_results = model.fit(X_train, \n",
    "                             Y_train, \n",
    "                             validation_data=(X_val, Y_val), \n",
    "                             shuffle=True, \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             epochs=EPOCHS, \n",
    "                             callbacks=base_callbacks)\n",
    "\n",
    "# Save Results as plot (2+)\n",
    "if (OUTPUT_LEVEL >= 1):\n",
    "    doc.plot_history(baseline_results, \n",
    "                     test_dir=base_folder, \n",
    "                     config_idx='base')\n",
    "\n",
    "# Evaluate Baseline Model and display results on all three sets \n",
    "print(\"Baseline Model Preformance:\")\n",
    "train_rpt = kt.ModelReport(X_train, Y_train, model, 'Training', print_report=True)\n",
    "val_rpt = kt.ModelReport(X_val, Y_val, model, 'Validation', print_report=True)\n",
    "test_rpt = kt.ModelReport(X_test, Y_test, model, 'Testing', print_report=True)\n",
    "\n",
    "print_s(None, \"Completed Baseline UNet Training.\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure Annotator Class\n",
    "The following class is used to  update the source annotation based on the UNet's predicted class map output. For time cost, all candidate geometries are preemptively generated with minimal metadata. \n",
    "\n",
    "This preloading process may take some time depending on your hardware. To reduce the memory cost, you can try reducing the total number of candidate geometries with the `pairs` attribute."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "annotator = Dynamic_Preloading_Annotator(\n",
    "    pairs=15,       # pairs: Number of candidates generated on either side \n",
    "    off_dist=1.5,   # of each shiftable vertex in the imperfect labels \n",
    "    interval=10,    \n",
    "    min_p=1e-02,\n",
    "    L=0.02,                \n",
    "    weight_buffer=2\n",
    ")\n",
    "# Preload Candidate Data \n",
    "initial_pmap = kt.Get_Pmap(source_raster=train_raster, pmodel=model)\n",
    "all_data = annotator.preload_candidates(imperfect_labels, initial_pmap)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EM Iteration\n",
    "The following block loops through fourteen steps of the EM algorithm. During the running, you'll be able to follow the progress of the UNet model's training and the annotation refinement through the cell's output. The same output will be saved to an indexed test folder under the directory specified by `RESULTS_DIR` in `config.py`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Begin Iteration\n",
    "print_s(None, \"Beginning Iteration, Target steps: 14\")\n",
    "for em_idx in range(0, 14): \n",
    "    # EM Step Initialization\n",
    "    # ----------------------\n",
    "    emfolder = os.path.join(test_dir, f'step_{em_idx:02}')\n",
    "    if not os.path.exists(emfolder): os.mkdir(emfolder)\n",
    "    print_s(em_idx, \"Started EM Step\")\n",
    "    \n",
    "    # Set first-iteration variables\n",
    "    if (em_idx == 0):\n",
    "        ### Evaluate original Shapefile Precision\n",
    "        source_iou = gt.gdf_iou(refined_labels, imperfect_labels)\n",
    "        prev_iou = source_iou\n",
    "        \n",
    "        # Stores EM results\n",
    "        em_dict = {\n",
    "            'Name': ['Base'],\n",
    "            'Test_Data': [test_rpt],\n",
    "            'Train_Data': [train_rpt],\n",
    "            'Val_Data': [val_rpt],\n",
    "            'Line_IoU': [np.round((source_iou*100), 2)],\n",
    "            'Epochs': [len(baseline_results.history['accuracy'])]\n",
    "        }\n",
    "        # Stores best values for this test\n",
    "        top_f1, top_f1_idx = 0, 0\n",
    "        top_iou, top_iou_idx = 0, 0\n",
    "        \n",
    "        buff_dist = 4\n",
    "        pmap_fp = os.path.join(emfolder, 'pmap_baseline.tif')\n",
    "    else:\n",
    "        buff_dist = 2\n",
    "        pmap_fp = os.path.join(emfolder, f'pmap_{em_idx:02}.tif')\n",
    "    \n",
    "    \n",
    "    # Override pmap output for output_level\n",
    "    \n",
    "    # Update Annotation\n",
    "    # -----------------\n",
    "    # Get predicted class map \n",
    "    predicted_class_map = kt.Get_Pmap(train_raster, model, pmap_fp)\n",
    "    print_s(em_idx, \"Generated Predicted Class Map from UNet model.\")\n",
    "\n",
    "    # Update annotation\n",
    "    annotation_fp = os.path.join(emfolder, f'annotation_{em_idx:02}.shp')\n",
    "    new_annotation = annotator.update_gdf_from_preload(all_data, class_map=predicted_class_map, out_path=annotation_fp) \n",
    "    print_s(em_idx, \"Created New Annotation.\")\n",
    "    \n",
    "    # Save iou for this annotation.\n",
    "    anno_iou = gt.gdf_iou(refined_labels, new_annotation)\n",
    "\n",
    "\n",
    "    # Create new Label Tensors\n",
    "    # ------------------------\n",
    "    # Buffer\n",
    "    buff_anno = gt.gdf_buffer(new_annotation, buff_dist=buff_dist, flatten=True)\n",
    "    \n",
    "    if (OUTPUT_LEVEL >= 2):\n",
    "        anno_raster_fp = os.path.join(emfolder, f'rasterized_annotation_{em_idx:02}.tif')\n",
    "    else: anno_raster_fp = None\n",
    "        \n",
    "    # Rasterize New Labels\n",
    "    anno_raster = gt.GDF_Rasterize(buff_anno, train_raster, out_path=anno_raster_fp)\n",
    "    \n",
    "    # Read Y_train, Y_val tensors from rasterized label\n",
    "    Y_train = tile.ResampleTiles(anno_raster, train_offsets_fp)\n",
    "    Y_val = tile.ResampleTiles(anno_raster, val_offsets_fp)\n",
    "    \n",
    "    # Upsample label tensors to match shape\n",
    "    Y_train = tile.AugmentImages(Y_train, h_flip=False, v_flip=True, rotate=True)\n",
    "    Y_val = tile.AugmentImages(Y_val, h_flip=False, v_flip=True, rotate=True)\n",
    "    print_s(em_idx, f\"Created Y_train {Y_train.shape} and Y_val {Y_val.shape}.\")\n",
    "    \n",
    "    if (OUTPUT_LEVEL >= 3):\n",
    "        # Save udpated label tenors\n",
    "        np.save(os.path.join(emfolder, f'Y_train_{em_idx:02}'), Y_train)\n",
    "        np.save(os.path.join(emfolder, f'Y_val_{em_idx:02}'), Y_val)\n",
    "    \n",
    "    # Re-Train UNet \n",
    "    # -------------\n",
    "    # Prepare optimizer, load callbacks \n",
    "    em_optimizer = Adam(lr=lr_schedule(em_idx), epsilon=1e-8, decay=1e-5)\n",
    "    callbacks = kt.SetCallbacks(weights_out=os.path.join(emfolder, f'unet_weights_{em_idx:02}.h5'), \n",
    "                                 tensorboard_path=os.path.join(emfolder, f'tensorboard_{em_idx:02}'),\n",
    "                                 output_level=OUTPUT_LEVEL)\n",
    "    \n",
    "    # Re-compile and Train Model\n",
    "    model = kt.Get_Model('UNET')\n",
    "    model.compile(optimizer=em_optimizer, \n",
    "                  loss=kt.dice_coef_loss, \n",
    "                  metrics=[kt.dice_coef,'accuracy', kt.f1_score])\n",
    "    training_history = model.fit(X_train, \n",
    "                                 Y_train, \n",
    "                                 validation_data=(X_val, Y_val), \n",
    "                                 shuffle=True, \n",
    "                                 batch_size=BATCH_SIZE, \n",
    "                                 epochs=EPOCHS, \n",
    "                                 callbacks=callbacks,\n",
    "                                 verbose=0)\n",
    "    print_s(em_idx, \"Completed model Training.\")\n",
    "    \n",
    "    # Save Training History \n",
    "    if (OUTPUT_LEVEL >= 1):\n",
    "        doc.plot_history(training_history, test_dir=emfolder, config_idx=em_idx)\n",
    "    \n",
    "    # Evaluate Model on train, validation, and test sets \n",
    "    if (OUTPUT_LEVEL >= 2):\n",
    "        # If last step saved to markdown, close the previous file \n",
    "        if hist_md: hist_md.close()\n",
    "        hist_markdown_fp = os.path.join(emfolder, f'history_{em_idx:02}.md')\n",
    "        hist_md = open(hist_markdown_fp, 'w+')\n",
    "    else: hist_md = None\n",
    "    \n",
    "    test_rpt = kt.ModelReport(X_test, Y_test, model, \"Testing\", index=(em_idx+1), report_md=hist_md)\n",
    "    train_rpt = kt.ModelReport(X_train, Y_train, model, \"Training\", index=(em_idx+1), report_md=hist_md)\n",
    "    val_rpt = kt.ModelReport(X_val, Y_val, model, \"Validation\", index=(em_idx+1), report_md=hist_md)\n",
    "        \n",
    "    # Summarize EM Step\n",
    "    # -----------------\n",
    "    # Record performance\n",
    "    em_dict['Name'].append(f'Step {em_idx:02}')\n",
    "    em_dict['Line_IoU'].append(np.round((anno_iou*100), 2))\n",
    "    em_dict['Epochs'].append(len(training_history.history['accuracy']))\n",
    "    em_dict['Test_Data'].append(test_rpt)\n",
    "    em_dict['Train_Data'].append(train_rpt)\n",
    "    em_dict['Val_Data'].append(val_rpt)\n",
    "    \n",
    "    # Update top scores for F1 and Annotation IoU\n",
    "    if test_rpt['F1_Score'] > top_f1:\n",
    "        top_f1 = test_rpt['F1_Score']\n",
    "        top_f1_idx = em_idx\n",
    "        top_f1_txt = \"(TOP)\"\n",
    "    else: top_f1_txt = \"\"\n",
    "    if np.round((anno_iou*100), 2) > top_iou:\n",
    "        top_iou = np.round((anno_iou*100), 2)\n",
    "        top_iou_idx = em_idx\n",
    "        top_iou_txt = \"(TOP)\"\n",
    "    else: top_iou_txt = \"\"\n",
    "    \n",
    "    # Print step data\n",
    "    print(f\"\\nEM Step {em_idx:02} Complete. [{gettime('%b %d | %I:%M:%S%p')}]\")\n",
    "    print(f'- Annotation IoU:     {(anno_iou*100):.2f}')\n",
    "    print(f'  - Source Improvement: {((anno_iou-source_iou)*100):+.2f}')\n",
    "    print(f'  - Step Improvement:   {((anno_iou-prev_iou)*100):+.2f}')\n",
    "    print('- Model Performance: (trained on new labels)')\n",
    "    print('  - Training Report (Step {em_idx:02}):')\n",
    "    doc.PrintReport(train_rpt)\n",
    "    print('  - Validation Report (Step {em_idx:02}):')\n",
    "    doc.PrintReport(val_rpt)\n",
    "    print('  - Testing Report (Step {em_idx:02}):')\n",
    "    doc.PrintReport(test_rpt)\n",
    "    print(\"----------------------------------\\n\\n\")\n",
    "    \n",
    "    # Increase iterator and save previous precision for step_delta\n",
    "    prev_iou = anno_iou"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Results**\n",
    "The following code is not mandatory, though it will help to understand the improvements made to the source annotation through the EM pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Results**: Plot F1, IoU\n",
    "The following block converts the model evaluation data saved in `em_dict` into pretty matplotlib plots. These are saved in the results folder and output below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Reformat EM Results and plot metrics by EM step\n",
    "model_dict = doc.get_model_dict(em_dict=em_dict)\n",
    "fig, axs = plt.subplots(2, 2, sharex=True, figsize=(16,10))\n",
    "doc.plot_axis(ax=axs[0,0], # Testing F1\n",
    "              data=model_dict['Test_Data']['F1_Score']*100, \n",
    "              name='Testing F1', \n",
    "              color_char='r', \n",
    "              symbol_char='s', \n",
    "              y_off=2)\n",
    "doc.plot_axis(ax=axs[0,1], # Training F1\n",
    "              data=model_dict['Train_Data']['F1_Score']*100, \n",
    "              name='Training F1', \n",
    "              color_char='g', \n",
    "              symbol_char='^', \n",
    "              y_off=2)\n",
    "doc.plot_axis(ax=axs[1,0], # Validation F1\n",
    "              data=model_dict['Val_Data']['F1_Score']*100, \n",
    "              name='Validation F1', \n",
    "              color_char='c', \n",
    "              symbol_char='^', \n",
    "              y_off=2)\n",
    "doc.plot_axis(ax=axs[1,1], #Annotation IoU\n",
    "              data=em_dict['Line_IoU'], \n",
    "              name='Line IoU', \n",
    "              color_char='m', \n",
    "              x_label='EM Step')\n",
    "    \n",
    "## Title and show, and save figure\n",
    "fig.suptitle(\"EM Test {:02}\".format(test_idx))\n",
    "fig_path = os.path.join(test_dir, 'results_plot_{:02}.png'.format(test_idx))\n",
    "fig.savefig(fig_path)\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Results**: Save Info to Markdown\n",
    "Most of the essential information printed for the EM pipeline is saved to a markdown file below. It contains model performance, annotation metrics, and  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Write test data to markdown\n",
    "markdown_fp = os.path.join(test_dir, 'info_{:02}.md'.format(test_idx))\n",
    "md = open(markdown_fp, 'w+')\n",
    "\n",
    "# Header\n",
    "md.write(\"# Geometric Annotation Errors - EM Test {:02}\\n\".format(test_idx))\n",
    "md.write(f\"### {gettime('%b %d | %I:%M:%S%p')}\\n\")\n",
    "md.write(\"\\n---\\n\\n\")\n",
    "# Results Section\n",
    "md.write(\"## **Results**:\\n\\n\")\n",
    "top_f1 = np.round((top_f1*100), 2)\n",
    "source_f1 = np.round((em_dict['Test_Data'][0]['F1_Score']*100), 2)\n",
    "source_iou = np.round(source_iou*100, 2)\n",
    "md.write(\"### Top Values:\\n\")\n",
    "md.write(f\" - Testing F1 Score: **{top_f1}** (`{top_f1-source_f1}`) - Step {top_f1_idx}\\n\")\n",
    "md.write(f\" - Annotation IoU: **{top_iou}** (`{top_iou-source_iou}`) - Step {top_iou_idx}\\n\\n\")\n",
    "\n",
    "# Write results by EM step\n",
    "md.write(\"### EM Iteration:\\n\")\n",
    "md.write(\"Step | Anno IoU | F1 | Epochs | LR | Train | Update\\n\")\n",
    "md.write(\"---- | -------- | -- | ------ | -- | ----- | ------\\n\")\n",
    "for idx in range(14):\n",
    "    if idx == 0:\n",
    "        md.write(\"{} | {} | {} | {} | {} | {} | {}\\n\".format(em_dict['Name'][idx], em_dict['Line_IoU'][idx], em_dict['Test_Data'][idx]['F1_Score'], em_dict['Epochs'][idx], em_dict['LR'][idx], em_dict['Training_Time'][idx], em_dict['Update_Time'][idx]))\n",
    "    else:\n",
    "        md.write(\"{} | {} (`{:+.2f}`) | {} | {} | {} | {} | {}\\n\".format(em_dict['Name'][idx], em_dict['Line_IoU'][idx], (em_dict['Line_IoU'][idx] - em_dict['Line_IoU'][0]), em_dict['Test_Data'][idx]['F1_Score'], em_dict['Epochs'][idx], em_dict['LR'][idx], em_dict['Training_Time'][idx], em_dict['Update_Time'][idx]))\n",
    "md.write(\"\\n\\n</br>\\n\\n\")\n",
    "md.write(\"### Model Performance:\\n\\n\")\n",
    "md.write(\"Step | Test F1 | Test (FP, FN) | Train F1 | Train (FP, FN) | Val F1 | Val (FP, FN) \\n\")\n",
    "md.write(\"---- | ------- | ------------- | -------- | -------------- | ------ | ------------ \\n\")\n",
    "for idx in range(0, 14):\n",
    "    # Create a string to hold this row's data\n",
    "    row_string = f\"{em_dict['Name'][idx]} | \"\n",
    "    for key in ['Test_Data', 'Train_Data', 'Val_Data']:\n",
    "        row_string += \"{:.2f} | ({:.2e}, {:.2e}) | \".format(em_dict[key][idx]['F1_Score']*100, em_dict[key][idx]['False_Positives'], em_dict[key][idx]['False_Negatives'])\n",
    "    md.write(row_string)\n",
    "md.close()\n",
    "print_s(None, f\"Results written to {markdown_fp}.\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_errors",
   "language": "python",
   "name": "geo_errors"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}