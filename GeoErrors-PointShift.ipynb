{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## GeometricAnnotationErrors - Point Shifting \n",
    "\n",
    "\n",
    "### Part 1: Loading Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Environment Configuration \"\"\"\n",
    "\n",
    "from  config import INPUT_DATA_DIR, TENSOR_DIR, RESULTS_DIR\n",
    "\n",
    "data_path = TENSOR_DIR\n",
    "source_path = INPUT_DATA_DIR\n",
    "out_root_dir = RESULTS_DIR\n",
    "\n",
    "# Pre-Load Determiner.\n",
    "preload = False\n",
    "# - True, the candidates are loaded and stored. \n",
    "# - False, the candidates are loaded from jupyter persistent storage\n",
    "\n",
    "\n",
    "\"\"\" Point Shifting Annotator Configuration \"\"\"\n",
    "\n",
    "# Weight consideration candidate0 line distance between candidate points\n",
    "length_weight_value = 0.02\n",
    "\n",
    "# Buffer in meters to apply to candidates\n",
    "weight_buffer = 2\n",
    "\n",
    "buff_dist = 4\n",
    "\n",
    "# Option to normalize over K^2 (True) or by K (False).\n",
    "normalize_full = False\n",
    "\n",
    "# Number of EM iterations\n",
    "em_target = 6\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Std Imports\n",
    "import os, sys, time, csv\n",
    "from datetime import datetime as dt \n",
    "\n",
    "# Module Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely.geometry as shp\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Lib imports\n",
    "import lib.Doc_Tools as doc\n",
    "import lib.GeoTools as gt\n",
    "import lib.Tiling as tile\n",
    "import lib.K_Tools as kt\n",
    "from lib import *\n",
    "\n",
    "# Seed environment\n",
    "tf.random.set_seed(2001)\n",
    "np.random.seed(2001)\n",
    "\n",
    "### Create Folder for test documentation\n",
    "test_idx, test_dir = doc.InitTest(out_root_dir,\n",
    "                                  em_target=em_target,\n",
    "                                  LR=learning_rate,\n",
    "                                  pairs=pairs,\n",
    "                                  interval=interval,\n",
    "                                  off_distance=off_dist,\n",
    "                                  min_p=min_probability)\n",
    "\n",
    "# Open Raster Imagery\n",
    "train_raster = rio.open(os.path.join(source_path, 'train_raster.tif'))\n",
    "test_raster  = rio.open(os.path.join(source_path, 'test_raster.tif'))\n",
    "# Open shapefiles, ensure correct CRS\n",
    "refined_labels  = gpd.read_file(os.path.join(source_path, 'refined_labels.shp'))\n",
    "refined_labels.to_crs(test_raster.crs, inplace=True)\n",
    "impefect_labels = gpd.read_file(os.path.join(source_path, 'imperfect_labels.shp'))\n",
    "impefect_labels.to_crs(train_raster.crs, inplace=True)\n",
    "\n",
    "### Loading Tensors and tile offsets\n",
    "X_train = np.load(os.path.join(data_path, 'X_train.npy'))\n",
    "Y_train = np.load(os.path.join(data_path, 'Y_train.npy'))\n",
    "X_val = np.load(os.path.join(data_path, 'X_val.npy'))\n",
    "Y_val = np.load(os.path.join(data_path, 'Y_val.npy'))\n",
    "X_test = np.load(os.path.join(data_path, 'X_test.npy'))\n",
    "Y_test = np.load(os.path.join(data_path, 'Y_test.npy'))\n",
    "train_offsets_fp = os.path.join(data_path, 'train_offsets.csv')\n",
    "val_offsets_fp = os.path.join(data_path, 'val_offsets.csv')\n",
    "print(\"Successfully loaded tensors.\")\n",
    "\n",
    "\n",
    "### Evaluate original Shapefile Precision\n",
    "source_iou = gt.gdf_iou(gt_labels, imperfect_labels)\n",
    "prev_iou = source_iou"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1: Baseline Model Training and Evaluation\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Baseline Model Training\n",
    "\"\"\" UNET Config \"\"\"\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "# Prepare Baseline Folder\n",
    "base_folder = os.path.join(test_dir, 'baseline')\n",
    "if not os.path.exists(base_folder): os.mkdir(base_folder)\n",
    "    \n",
    "# Prepare Callbacks, including weight output\n",
    "base_callbacks = kt.SetCallbacks(weights_out=os.path.join(base_folder, 'BaselineWeights.h5'),\n",
    "                                 tensorboard_path=os.path.join(base_folder, 'tensorboard'))\n",
    "\n",
    "# Create Optimizer \n",
    "optimizer = Adam(lr=learning_rate, epsilon=1e-8, decay=1e-5)\n",
    "\n",
    "# Select and Build Model\n",
    "model = kt.Get_Model('UNET')\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss = kt.dice_coef_loss, \n",
    "              metrics=[kt.dice_coef,'accuracy', kt.f1_score])\n",
    "\n",
    "# Train Model\n",
    "baseline_results = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), shuffle=True, batch_size=batch_size, epochs=epochs, callbacks=base_callbacks)\n",
    "\n",
    "# Save History plot and csv\n",
    "doc.plot_history(baseline_results, test_dir=base_folder, config_idx='base')\n",
    "\n",
    "epochs_used = len(baseline_results.history['accuracy'])\n",
    "\n",
    "\"\"\" Evaluate Baseline Model Preformance \"\"\"\n",
    "print(\"Baseline Preformance:\")\n",
    "train_rpt = kt.ModelReport(X_train, Y_train, model, 'Training')\n",
    "val_rpt = kt.ModelReport(X_val, Y_val, model, 'Validation')\n",
    "test_rpt = kt.ModelReport(X_test, Y_test, model, 'Testing')\n",
    "\n",
    "print(dt.now().strftime('\\n\\n%a at %I:%M:%S%p'))\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 2: EM Iteration\n",
    "\n",
    "`Warning`: By proceeding, the EM Iteration will use the model configured above with the parameters already set. Tune the baseline model above as many times as needed before proceeding.\n",
    "\n",
    "#### Section 1: Configure EM Test with Annotator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seg_class_map_fp = None\n",
    "preload = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prepare annotator\n",
    "annotator = Dynamic_Preloading_Annotator(pairs=15,                   \n",
    "                                         off_dist=1.5,             \n",
    "                                         interval=10,             \n",
    "                                         min_p=1e-02,                   \n",
    "                                         L=0.02,                        \n",
    "                                         weight_buffer=4,   \n",
    "                                         normalize_full=normalize_full, \n",
    "                                         )\n",
    "\n",
    "# Preload Candidate Data \n",
    "print(\"PRELOADING\")\n",
    "initial_pmap = kt.Get_Pmap(source_raster=train_raster, pmodel=model, pmap_fp=None)\n",
    "all_data = annotator.preload_candidates(imperfect_labels, initial_pmap)\n",
    "    \n",
    "        \n",
    "# Prepare performance storage\n",
    "em_dict = {\n",
    "    'Name': ['Base'],\n",
    "    'Test_Data': [test_rpt],\n",
    "    'Train_Data': [train_rpt],\n",
    "    'Val_Data': [val_rpt],\n",
    "    'Line_IoU': [np.round((source_iou*100), 2)],\n",
    "    'Epochs': [epochs_used],\n",
    "}\n",
    "\n",
    "# Prepare EM iterator index.\n",
    "EM_iterator = 0\n",
    "\n",
    "# Prepare storage for top F1 and IoU\n",
    "top_f1, top_f1_idx = 0, 0\n",
    "top_iou, top_iou_idx = 0, 0\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "buff_dist = 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run EM Iteration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Beginning Iteration, Target steps:\", em_target)\n",
    "\n",
    "while EM_iterator < em_target: \n",
    "    \n",
    "    # 0. Initialization\n",
    "    # ---------------------------\n",
    "\n",
    "    # Create folder for em step\n",
    "    emfolder = os.path.join(test_dir, 'step_{:02}'.format(EM_iterator))\n",
    "    if not os.path.exists(emfolder): os.mkdir(emfolder)\n",
    "\n",
    "    print(\"\\nEM Step {:02} begun.\".format(EM_iterator))\n",
    "    print(dt.now().strftime('%a at %I:%M:%S%p'))\n",
    "\n",
    "    \n",
    "    # 1. Update Annotations\n",
    "    # ---------------------------\n",
    "\n",
    "    # Get predicted class map \n",
    "    if (EM_iterator == 0):\n",
    "        pmap_fp = os.path.join(emfolder, 'pmap_baseline.tif')\n",
    "    else:\n",
    "        pmap_fp = os.path.join(emfolder, 'pmap_{:02}.tif'.format(EM_iterator))\n",
    "    predicted_class_map = kt.Get_Pmap(train_raster, model, pmap_fp)\n",
    "    print(\"\\nGenerated Predicted Class and Intermediate Feature Maps from previous model. (Step {:02})\".format(EM_iterator))\n",
    "    print(dt.now().strftime('%a at %I:%M:%S%p'))\n",
    "    \n",
    "\n",
    "    # Update annotation\n",
    "    annotation_fp = os.path.join(emfolder, 'annotation_{:02}.shp'.format(EM_iterator))\n",
    "    new_annotation = annotator.update_gdf_from_preload(all_data, class_map=predicted_class_map, out_path=annotation_fp) \n",
    "\n",
    "    print(\"\\nCreated New Annotation. (Step {:02})\".format(EM_iterator))\n",
    "    print(dt.now().strftime('%a at %I:%M:%S%p'))\n",
    "\n",
    "    # Generate and save all considered point groups\n",
    "    candidate_fp = os.path.join(emfolder, 'candidates_{:02}.shp'.format(EM_iterator))\n",
    "    annotator.get_candidates(imperfect_labels, class_map=predicted_class_map, out_path=candidate_fp)\n",
    "    \n",
    "    # Save iou for this annotation.\n",
    "    anno_iou = gt.gdf_iou(gt_labels, new_annotation)\n",
    "\n",
    "\n",
    "    # 2. Create new Label Tensors\n",
    "    # ---------------------------\n",
    "    \n",
    "    # 2.1 Rasterize New Labels\n",
    "    buff_anno = gt.gdf_buffer(new_annotation, buff_dist=buff_dist, flatten=True)\n",
    "    anno_raster_fp = os.path.join(emfolder, 'rasterized_annotation_{:02}.tif'.format(EM_iterator))\n",
    "    anno_raster = gt.GDF_Rasterize(buff_anno, train_raster, out_path=anno_raster_fp)\n",
    "    \n",
    "    # Read Y_train, Y_val tensors from rasterized label\n",
    "    Y_train = tile.ResampleTiles(anno_raster, train_offsets_fp)\n",
    "    Y_val = tile.ResampleTiles(anno_raster, val_offsets_fp)\n",
    "    \n",
    "    # Upsample label tensors to match shape\n",
    "    Y_train = tile.AugmentImages(Y_train, h_flip=False, v_flip=True, rotate=True)\n",
    "    Y_val = tile.AugmentImages(Y_val, h_flip=False, v_flip=True, rotate=True)\n",
    "    \n",
    "    print(\"\\nCreated Y_train {} and Y_val {}. (Step {:02})\".format(Y_train.shape, Y_val.shape, EM_iterator))\n",
    "    print(dt.now().strftime('%a at %I:%M:%S%p'))\n",
    "    \n",
    "    \n",
    "    # 3. Re-Train U-Net \n",
    "    # ---------------------------\n",
    "    \n",
    "    # Load Callbacks \n",
    "    callbacks =  kt.SetCallbacks(weights_out=os.path.join(emfolder, 'unet_weights_{:02}.h5'.format(EM_iterator)), \n",
    "                                 tensorboard_path=os.path.join(emfolder, 'tensorboard_{:02}'.format(EM_iterator)))\n",
    "    \n",
    "    # Re-compile and Train Model\n",
    "    model = kt.Get_Model('UNET')\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=kt.dice_coef_loss, \n",
    "                  metrics=[kt.dice_coef,'accuracy', kt.f1_score])\n",
    "    training_history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), shuffle=True, batch_size=batch_size, epochs=epochs, callbacks=callbacks, verbose=0)\n",
    "    print(\"Completed model Training. (Step {:02})\".format(EM_iterator))\n",
    "    print(dt.now().strftime('%a at %I:%M:%S%p'))\n",
    "    \n",
    "\n",
    "    # 4. Evaluate Model\n",
    "    # ---------------------------\n",
    "\n",
    "    # Save History Plots and CSV\n",
    "    doc.plot_history(training_history, test_dir=emfolder, config_idx=EM_iterator)\n",
    "    \n",
    "    hist_markdown_fp = os.path.join(emfolder, 'history_{:02}.md'.format(EM_iterator))\n",
    "    with open(hist_markdown_fp, 'w+') as hist_md:\n",
    "        test_rpt = kt.ModelReport(X_test, Y_test, model, \"Testing\", index=(EM_iterator+1), report_md=hist_md)\n",
    "        train_rpt = kt.ModelReport(X_train, Y_train, model, \"Training\", index=(EM_iterator+1), report_md=hist_md)\n",
    "        val_rpt = kt.ModelReport(X_val, Y_val, model, \"Validation\", index=(EM_iterator+1), report_md=hist_md)\n",
    "    \n",
    "    # Record performance\n",
    "    em_dict['Name'].append('Step {:02}'.format(EM_iterator))\n",
    "    em_dict['Line_IoU'].append(np.round((anno_iou*100), 2))\n",
    "    em_dict['Epochs'].append(len(training_history.history['accuracy']))\n",
    "    em_dict['Test_Data'].append(test_rpt)\n",
    "    em_dict['Train_Data'].append(train_rpt)\n",
    "    em_dict['Val_Data'].append(val_rpt)\n",
    "    \n",
    "    \n",
    "    # Update top F1 and Annotation IoU\n",
    "    if test_rpt['F1_Score'] > top_f1:\n",
    "        top_f1 = test_rpt['F1_Score']\n",
    "        top_f1_idx = EM_iterator\n",
    "        print(\"\\nNew Top F1: {:.2f}\".format(top_f1*100))\n",
    "    if np.round((anno_iou*100), 2) > top_iou:\n",
    "        top_iou = np.round((anno_iou*100), 2)\n",
    "        top_iou_idx = EM_iterator\n",
    "        print(\"\\nNew Top IoU: {:.2f}\".format(top_iou))\n",
    "    \n",
    "    # Print step data\n",
    "    print(\"\\nEM Step ({:02}) Complete on {}\".format(EM_iterator, dt.now().strftime('%a at %I:%M:%S%p')))\n",
    "    print('- Annotation IoU:     {:.2f}'.format(anno_iou*100))\n",
    "    print('  - Source Improvement: {:+.2f}'.format((anno_iou-source_iou)*100))\n",
    "    print('  - Step Improvement:   {:+.2f}'.format((anno_iou-prev_iou)*100))\n",
    "    print('- Model Performance: (trained on new labels)')\n",
    "    print('  - Training:')\n",
    "    kt.PrintReport(train_rpt)\n",
    "    print('  - Validation:')\n",
    "    kt.PrintReport(val_rpt)\n",
    "    print('  - Testing:')\n",
    "    kt.PrintReport(test_rpt)\n",
    "    print(\"----------------------------------\\n\\n\")\n",
    "    \n",
    "    # Increase iterator and save previous precision for step_delta\n",
    "    EM_iterator += 1\n",
    "    prev_iou = anno_iou\n",
    " \n",
    "# Increase EM target for optional subsequent runs\n",
    "em_target += 1"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Clean data for easy indexing\n",
    "\n",
    "## Converts each of the report lists into a dict of lists for each value\n",
    "model_dict = {'Test_Data': {}, 'Train_Data': {}, 'Val_Data': {}}\n",
    "for em_key in model_dict.keys():\n",
    "    for report in em_dict[em_key]:\n",
    "        for rpt_key, rpt_value in [(key, item) for key, item in report.items()]:\n",
    "            if rpt_key not in model_dict[em_key].keys():\n",
    "                model_dict[em_key].update({rpt_key: np.array([rpt_value])})\n",
    "            else:\n",
    "                model_dict[em_key][rpt_key] = np.append(model_dict[em_key][rpt_key], report[rpt_key])\n",
    "\n",
    "\n",
    "# Create Figure for Plots\n",
    "fig, axs = plt.subplots(2, 2, sharex=True, figsize=(16,10))\n",
    "\n",
    "## Plot Testing F1\n",
    "doc.plot_axis(ax=axs[0,0], \n",
    "              data=model_dict['Test_Data']['F1_Score']*100, \n",
    "              name='Testing F1', \n",
    "              color_char='r', \n",
    "              symbol_char='s', \n",
    "              y_off=2,\n",
    "              label_delta=False)\n",
    "\n",
    "## Plot Training F1\n",
    "doc.plot_axis(ax=axs[0,1], \n",
    "              data=model_dict['Train_Data']['F1_Score']*100, \n",
    "              name='Training F1', \n",
    "              color_char='g', \n",
    "              symbol_char='^', \n",
    "              y_off=2,\n",
    "              label_delta=False)\n",
    "\n",
    "## Plot Validation F1\n",
    "doc.plot_axis(ax=axs[1,0], \n",
    "              data=model_dict['Val_Data']['F1_Score']*100, \n",
    "              name='Validation F1', \n",
    "              color_char='c', \n",
    "              symbol_char='^', \n",
    "              y_off=2,\n",
    "              label_delta=False)\n",
    "\n",
    "## Plot Annotation IoU\n",
    "doc.plot_axis(ax=axs[1,1], \n",
    "              data=em_dict['Line_IoU'], \n",
    "              name='Line IoU', \n",
    "              color_char='m', \n",
    "              x_label='EM Step',\n",
    "              label_delta=False)\n",
    "    \n",
    "## Title and show, and save figure\n",
    "fig.suptitle(\"EM Test {:02}\".format(test_idx))\n",
    "fig_path = os.path.join(test_dir, 'test_{:02}_plot.png'.format(test_idx))\n",
    "fig.savefig(fig_path)\n",
    "fig.show()\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save Info to Markdown\n",
    "\n",
    "Saves:\n",
    "- Annotator Config\n",
    "- Model Config\n",
    "- EM preformance table\n",
    "- Model preformance table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Write test data to markdown\n",
    "markdown_fp = os.path.join(test_dir, 'em_test_info_{:02}.md'.format(test_idx))\n",
    "md = open(markdown_fp, 'w+')\n",
    "\n",
    "# Header / Notes\n",
    "md.write(\"# Geometric Annotation Errors - EM Test {:02}\\n\".format(test_idx))\n",
    "md.write(dt.now().strftime('### %a at %I:%M:%S%p\\n'))\n",
    "md.write(\"\\n---\\n\\n\")\n",
    "\n",
    "\n",
    "# Results Section\n",
    "md.write(\"## **Results**:\\n\\n\")\n",
    "# Write best scores \n",
    "top_f1 = np.round((top_f1*100), 2)\n",
    "source_f1 = np.round((em_dict['Test_Data'][0]['F1_Score']*100), 2)\n",
    "source_iou = np.round(source_iou*100, 2)\n",
    "md.write(\"### Top Values:\\n\")\n",
    "md.write(f\" - Testing F1 Score: **{top_f1}** (`{top_f1-source_f1}`) - Step {top_f1_idx}\\n\")\n",
    "md.write(f\" - Annotation IoU: **{top_iou}** (`{top_iou-source_iou}`) - Step {top_iou_idx}\\n\\n\")\n",
    "\n",
    "# Write results by EM step\n",
    "md.write(\"### EM Iteration:\\n\")\n",
    "md.write(\"Step | Anno IoU | F1 | Epochs | LR | Train | Update\\n\")\n",
    "md.write(\"---- | -------- | -- | ------ | -- | ----- | ------\\n\")\n",
    "for idx in range(em_target):\n",
    "    if idx == 0:\n",
    "        md.write(\"{} | {} | {} | {} | {} | {} | {}\\n\".format(em_dict['Name'][idx], em_dict['Line_IoU'][idx], em_dict['Test_Data'][idx]['F1_Score'], em_dict['Epochs'][idx], em_dict['LR'][idx], em_dict['Training_Time'][idx], em_dict['Update_Time'][idx]))\n",
    "    else:\n",
    "        md.write(\"{} | {} (`{:+.2f}`) | {} | {} | {} | {} | {}\\n\".format(em_dict['Name'][idx], em_dict['Line_IoU'][idx], (em_dict['Line_IoU'][idx] - em_dict['Line_IoU'][0]), em_dict['Test_Data'][idx]['F1_Score'], em_dict['Epochs'][idx], em_dict['LR'][idx], em_dict['Training_Time'][idx], em_dict['Update_Time'][idx]))\n",
    "md.write(\"\\n\\n</br>\\n\\n\")\n",
    "md.write(\"### Model Performance:\\n\\n\")\n",
    "md.write(\"Step | Test F1 | Test (FP, FN) | Train F1 | Train (FP, FN) | Val F1 | Val (FP, FN) \\n\")\n",
    "md.write(\"---- | ------- | ------------- | -------- | -------------- | ------ | ------------ \\n\")\n",
    "for idx in range(em_target):\n",
    "    # Create a string to hold this row's data\n",
    "    row_string = f\"{em_dict['Name'][idx]} | \"\n",
    "    for key in ['Test_Data', 'Train_Data', 'Val_Data']:\n",
    "        row_string += \"{:.2f} | ({:.2e}, {:.2e}) | \".format(em_dict[key][idx]['F1_Score']*100, em_dict[key][idx]['False_Positives'], em_dict[key][idx]['False_Negatives'])\n",
    "    md.write(row_string)\n",
    "    \n",
    "# Close Markdown\n",
    "md.close()\n",
    "\n",
    "print(f\"Results written to {markdown_fp}.\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_errors",
   "language": "python",
   "name": "geo_errors"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}